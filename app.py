import os
os.environ["TF_USE_LEGACY_KERAS"] = "1"

import streamlit as st
import cv2
import numpy as np
import pandas as pd
from datetime import datetime
from PIL import Image
from deepface import DeepFace
from streamlit_webrtc import webrtc_streamer, VideoProcessorBase, RTCConfiguration, WebRtcMode
import av
import plotly.express as px
from fpdf import FPDF

st.set_page_config(page_title="Emotion AI", layout="wide")
st.title("üòä Real-Time Emotion Detection & Analytics")

if 'data' not in st.session_state:
    st.session_state.data = pd.DataFrame(columns=['Time', 'Emotion'])

# --- Emoji Loading ---
@st.cache_resource
def load_emojis():
    emoji_map = {
        'happy': 'happy.png', 'sad': 'sad.png', 'angry': 'angry.png',
        'surprise': 'surprise.png', 'neutral': 'neutral.png',
        'disgust': 'disgust.png', 'fear': 'fear.png'
    }
    imgs = {}
    for k, v in emoji_map.items():
        try:
            img = Image.open(v).convert("RGBA").resize((100, 100))
            imgs[k] = np.array(img)
        except: continue
    return imgs

emojis = load_emojis()

def overlay_emoji(frame, emotion):
    if emotion in emojis:
        emoji = emojis[emotion]
        h, w, _ = frame.shape
        y1, y2, x1, x2 = 20, 120, w-120, w-20
        alpha_s = emoji[:, :, 3] / 255.0
        alpha_l = 1.0 - alpha_s
        for c in range(0, 3):
            frame[y1:y2, x1:x2, c] = (alpha_s * emoji[:, :, c] + alpha_l * frame[y1:y2, x1:x2, c])
    return frame

# --- Processor ---
class EmotionProcessor(VideoProcessorBase):
    def __init__(self):
        self.current_emotion = "neutral"
        self.frame_count = 0

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        self.frame_count += 1
        
        if self.frame_count % 15 == 0:
            try:
                res = DeepFace.analyze(img, actions=['emotion'], enforce_detection=False, detector_backend='opencv')
                self.current_emotion = res[0]['dominant_emotion']
                now = datetime.now().strftime("%H:%M:%S")
                new_row = pd.DataFrame({'Time': [now], 'Emotion': [self.current_emotion]})
                st.session_state.data = pd.concat([st.session_state.data, new_row], ignore_index=True)
            except: pass

        img = overlay_emoji(img, self.current_emotion)
        cv2.putText(img, f"Emotion: {self.current_emotion}", (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        return av.VideoFrame.from_ndarray(img, format="bgr24")

# --- Streamer ---
ctx = webrtc_streamer(
    key="emotion-analysis",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration=RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]}),
    video_processor_factory=EmotionProcessor,
    media_stream_constraints={"video": True, "audio": False},
)

# --- Post-Session Report ---
if not ctx.state.playing and not st.session_state.data.empty:
    st.divider()
    st.header("üìä Emotion Analytics Report")
    df = st.session_state.data
    
    col1, col2 = st.columns(2)
    with col1:
        st.plotly_chart(px.bar(df['Emotion'].value_counts()), use_container_width=True)
    with col2:
        st.plotly_chart(px.pie(df, names='Emotion'), use_container_width=True)

    # PDF Report
    pdf = FPDF()
    pdf.add_page()
    pdf.set_font("Arial", size=12)
    pdf.cell(200, 10, txt="Emotion Session Report", ln=True, align='C')
    for i, row in df.tail(20).iterrows():
        pdf.cell(200, 10, txt=f"{row['Time']}: {row['Emotion']}", ln=True)
    
    pdf_out = pdf.output(dest='S').encode('latin-1')
    st.download_button("‚¨áÔ∏è Download PDF", pdf_out, "report.pdf", "application/pdf")

st.caption("generated by pulak saha")
